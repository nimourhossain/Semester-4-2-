{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16d56ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found: input_data.xlsx. Reading attempt started...\n",
      "Warning: CSV ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶™‡ßú‡¶æ ‡¶Ø‡¶æ‡¶ö‡ßç‡¶õ‡ßá ‡¶®‡¶æ‡•§ ‡¶è‡¶ü‡¶ø ‡¶∏‡¶Æ‡ßç‡¶≠‡¶¨‡¶§ ‡¶è‡¶ï‡¶ü‡¶ø Excel (.xlsx) ‡¶´‡¶æ‡¶á‡¶≤‡•§\n",
      "Excel ‡¶´‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü‡ßá ‡¶™‡ßú‡¶æ‡¶∞ ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶ö‡ßç‡¶õ‡ßá...\n",
      "Success! ‡¶´‡¶æ‡¶á‡¶≤‡¶ü‡¶ø ‡¶Ü‡¶∏‡¶≤‡ßá ‡¶è‡¶ï‡¶ü‡¶ø Excel ‡¶´‡¶æ‡¶á‡¶≤ ‡¶õ‡¶ø‡¶≤‡•§ ‡¶∏‡¶´‡¶≤‡¶≠‡¶æ‡¶¨‡ßá ‡¶™‡ßú‡¶æ ‡¶π‡ßü‡ßá‡¶õ‡ßá‡•§\n",
      "Columns found: ['english', 'standard bangla', 'garo', 'marma', 'chakma']\n",
      "Original sentences: 292\n",
      "Generating new sentences...\n",
      "\n",
      "[SUCCESS] ‡¶ï‡¶æ‡¶ú ‡¶∏‡¶Æ‡ßç‡¶™‡¶®‡ßç‡¶®! 2000 ‡¶ü‡¶ø ‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡ßá‡¶∞ ‡¶´‡¶æ‡¶á‡¶≤ ‡¶§‡ßà‡¶∞‡¶ø ‡¶π‡ßü‡ßá‡¶õ‡ßá‡•§\n",
      "‡¶´‡¶æ‡¶á‡¶≤‡ßá‡¶∞ ‡¶®‡¶æ‡¶Æ: marma_dataset_2000.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# --- ‡¶´‡¶æ‡¶á‡¶≤ ‡¶∏‡ßá‡¶ü‡¶Ü‡¶™ ---\n",
    "filename = 'input_data.xlsx'\n",
    "\n",
    "# ‡¶´‡¶æ‡¶á‡¶≤ ‡¶Ü‡¶õ‡ßá ‡¶ï‡¶ø‡¶®‡¶æ ‡¶ö‡ßá‡¶ï\n",
    "if not os.path.exists(filename):\n",
    "    print(f\"Error: '{filename}' ‡¶´‡¶æ‡¶á‡¶≤‡¶ü‡¶ø ‡¶™‡¶æ‡¶ì‡ßü‡¶æ ‡¶Ø‡¶æ‡¶ö‡ßç‡¶õ‡ßá ‡¶®‡¶æ!\")\n",
    "    # ‡¶è‡¶ï‡ßç‡¶∏‡¶ø‡¶ü ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø\n",
    "    raise SystemExit(\"Stop: File not found.\")\n",
    "\n",
    "print(f\"File found: {filename}. Reading attempt started...\")\n",
    "\n",
    "df = None\n",
    "\n",
    "# --- ‡¶ß‡¶æ‡¶™ ‡ßß: CSV ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶™‡ßú‡¶æ‡¶∞ ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ (‡¶¨‡¶ø‡¶≠‡¶ø‡¶®‡ßç‡¶® ‡¶è‡¶®‡¶ï‡ßã‡¶°‡¶ø‡¶Ç ‡¶¶‡¶ø‡ßü‡ßá) ---\n",
    "encodings = ['utf-8', 'utf-8-sig', 'latin1', 'cp1252']\n",
    "for enc in encodings:\n",
    "    try:\n",
    "        df = pd.read_csv(filename, encoding=enc)\n",
    "        print(f\"Success! CSV ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶™‡ßú‡¶æ ‡¶ó‡ßá‡¶õ‡ßá (Encoding: {enc})\")\n",
    "        break\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# --- ‡¶ß‡¶æ‡¶™ ‡ß®: ‡¶Ø‡¶¶‡¶ø CSV ‡¶´‡ßá‡¶≤ ‡¶ï‡¶∞‡ßá, ‡¶§‡¶¨‡ßá EXCEL ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶™‡ßú‡¶æ‡¶∞ ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ ---\n",
    "if df is None:\n",
    "    print(\"Warning: CSV ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶™‡ßú‡¶æ ‡¶Ø‡¶æ‡¶ö‡ßç‡¶õ‡ßá ‡¶®‡¶æ‡•§ ‡¶è‡¶ü‡¶ø ‡¶∏‡¶Æ‡ßç‡¶≠‡¶¨‡¶§ ‡¶è‡¶ï‡¶ü‡¶ø Excel (.xlsx) ‡¶´‡¶æ‡¶á‡¶≤‡•§\")\n",
    "    print(\"Excel ‡¶´‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü‡ßá ‡¶™‡ßú‡¶æ‡¶∞ ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶ö‡ßç‡¶õ‡ßá...\")\n",
    "    \n",
    "    try:\n",
    "        # ‡¶è‡¶ï‡ßç‡¶∏‡ßá‡¶≤ ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶™‡ßú‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶´‡¶æ‡¶á‡¶≤‡ßá‡¶∞ ‡¶®‡¶æ‡¶Æ .xlsx ‡¶π‡¶§‡ßá ‡¶π‡ßü, ‡¶§‡¶æ‡¶á ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶ï‡¶™‡¶ø ‡¶ï‡¶∞‡ßá ‡¶®‡¶æ‡¶Æ ‡¶¨‡¶¶‡¶≤‡¶æ‡¶¨\n",
    "        temp_excel_name = 'temp_input_file.xlsx'\n",
    "        shutil.copy(filename, temp_excel_name)\n",
    "        \n",
    "        # ‡¶è‡¶¨‡¶æ‡¶∞ ‡¶è‡¶ï‡ßç‡¶∏‡ßá‡¶≤ ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶™‡ßú‡¶æ‡¶∞ ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ\n",
    "        df = pd.read_excel(temp_excel_name)\n",
    "        print(\"Success! ‡¶´‡¶æ‡¶á‡¶≤‡¶ü‡¶ø ‡¶Ü‡¶∏‡¶≤‡ßá ‡¶è‡¶ï‡¶ü‡¶ø Excel ‡¶´‡¶æ‡¶á‡¶≤ ‡¶õ‡¶ø‡¶≤‡•§ ‡¶∏‡¶´‡¶≤‡¶≠‡¶æ‡¶¨‡ßá ‡¶™‡ßú‡¶æ ‡¶π‡ßü‡ßá‡¶õ‡ßá‡•§\")\n",
    "        \n",
    "        # ‡¶ü‡ßá‡¶Æ‡ßç‡¶™ ‡¶´‡¶æ‡¶á‡¶≤ ‡¶°‡¶ø‡¶≤‡¶ø‡¶ü\n",
    "        os.remove(temp_excel_name)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n[CRITICAL ERROR] ‡¶´‡¶æ‡¶á‡¶≤‡¶ü‡¶ø ‡¶ï‡ßã‡¶®‡ßã‡¶≠‡¶æ‡¶¨‡ßá‡¶á ‡¶™‡ßú‡¶æ ‡¶Ø‡¶æ‡¶ö‡ßç‡¶õ‡ßá ‡¶®‡¶æ‡•§ Error: {e}\")\n",
    "        print(\"‡¶™‡¶∞‡¶æ‡¶Æ‡¶∞‡ßç‡¶∂: ‡¶Æ‡ßÇ‡¶≤ ‡¶è‡¶ï‡ßç‡¶∏‡ßá‡¶≤ ‡¶´‡¶æ‡¶á‡¶≤‡¶ü‡¶ø ‡¶ñ‡ßÅ‡¶≤‡ßÅ‡¶® ‡¶è‡¶¨‡¶Ç 'Save As' -> 'CSV (Comma delimited)' ‡¶´‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü‡ßá ‡¶∏‡ßá‡¶≠ ‡¶ï‡¶∞‡ßá ‡¶Ü‡¶¨‡¶æ‡¶∞ ‡¶Ü‡¶™‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶®‡•§\")\n",
    "        raise SystemExit(\"Stop: Cannot read file.\")\n",
    "\n",
    "# --- ‡¶ß‡¶æ‡¶™ ‡ß©: ‡¶ï‡¶≤‡¶æ‡¶Æ ‡¶†‡¶ø‡¶ï ‡¶ï‡¶∞‡¶æ ‡¶è‡¶¨‡¶Ç ‡¶°‡¶æ‡¶ü‡¶æ ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏‡¶ø‡¶Ç ---\n",
    "# ‡¶ï‡¶≤‡¶æ‡¶Æ ‡¶®‡¶æ‡¶Æ ‡¶ï‡ßç‡¶≤‡¶ø‡¶® ‡¶ï‡¶∞‡¶æ\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "# ‡¶ï‡¶≤‡¶æ‡¶Æ ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶™ ‡¶ï‡¶∞‡¶æ (English, Bangla, Marma)\n",
    "# ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶´‡¶æ‡¶á‡¶≤‡ßá ‡¶ï‡¶≤‡¶æ‡¶Æ‡ßá‡¶∞ ‡¶∏‡¶Æ‡ßç‡¶≠‡¶æ‡¶¨‡ßç‡¶Ø ‡¶®‡¶æ‡¶Æ‡¶ó‡ßÅ‡¶≤‡ßã ‡¶ñ‡ßÅ‡¶Å‡¶ú‡¶¨\n",
    "cols = df.columns.tolist()\n",
    "print(\"Columns found:\", cols)\n",
    "\n",
    "english_col = next((c for c in cols if 'english' in c), None)\n",
    "bangla_col = next((c for c in cols if 'bangla' in c or 'bengali' in c), None)\n",
    "marma_col = next((c for c in cols if 'marma' in c), None)\n",
    "\n",
    "if english_col and bangla_col and marma_col:\n",
    "    df_clean = df[[english_col, bangla_col, marma_col]].dropna()\n",
    "    df_clean.columns = ['English', 'Bangla', 'Marma']\n",
    "else:\n",
    "    # ‡¶®‡¶æ‡¶Æ ‡¶®‡¶æ ‡¶Æ‡¶ø‡¶≤‡¶≤‡ßá ‡¶á‡¶®‡¶°‡ßá‡¶ï‡ßç‡¶∏ ‡¶ß‡¶∞‡ßá (0, 1, 3 - ‡¶ß‡¶æ‡¶∞‡¶£‡¶æ ‡¶ï‡¶∞‡¶õ‡¶ø)\n",
    "    print(\"Warning: ‡¶ï‡¶≤‡¶æ‡¶Æ‡ßá‡¶∞ ‡¶®‡¶æ‡¶Æ ‡¶Æ‡¶ø‡¶≤‡¶õ‡ßá ‡¶®‡¶æ‡•§ ‡¶á‡¶®‡¶°‡ßá‡¶ï‡ßç‡¶∏ ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡ßü‡ßÄ ‡¶ï‡¶≤‡¶æ‡¶Æ ‡¶®‡ßá‡¶ì‡ßü‡¶æ ‡¶π‡¶ö‡ßç‡¶õ‡ßá...\")\n",
    "    # ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£‡¶§ ‡ß®‡ßü, ‡ß©‡ßü ‡¶¨‡¶æ ‡ß™‡¶∞‡ßç‡¶• ‡¶ï‡¶≤‡¶æ‡¶Æ‡ßá ‡¶Æ‡¶æ‡¶∞‡¶Æ‡¶æ ‡¶•‡¶æ‡¶ï‡ßá‡•§ ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶´‡¶æ‡¶á‡¶≤ ‡¶¶‡ßá‡¶ñ‡ßá ‡¶Æ‡¶®‡ßá ‡¶π‡¶ö‡ßç‡¶õ‡ßá:\n",
    "    # 0=English, 1=Bangla, 3=Marma (‡¶è‡¶ü‡¶ø ‡¶´‡¶æ‡¶á‡¶≤‡ßá‡¶∞ ‡¶ó‡¶†‡¶® ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡ßü‡ßÄ ‡¶¨‡¶¶‡¶≤‡¶æ‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá)\n",
    "    try:\n",
    "        df_clean = df.iloc[:, [0, 1, 3]].dropna() \n",
    "        df_clean.columns = ['English', 'Bangla', 'Marma']\n",
    "    except:\n",
    "         print(\"Error: ‡¶ï‡¶≤‡¶æ‡¶Æ ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßá ‡¶™‡¶æ‡¶ì‡ßü‡¶æ ‡¶Ø‡¶æ‡¶ö‡ßç‡¶õ‡ßá ‡¶®‡¶æ‡•§\")\n",
    "         raise SystemExit(\"Stop: Column mismatch.\")\n",
    "\n",
    "# --- ‡¶ß‡¶æ‡¶™ ‡ß™: ‡ß®‡ß¶‡ß¶‡ß¶ ‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø ‡¶§‡ßà‡¶∞‡¶ø (Augmentation) ---\n",
    "def augment_text(text):\n",
    "    text = str(text)\n",
    "    words = text.split()\n",
    "    if len(words) < 3: return text\n",
    "    \n",
    "    if random.random() > 0.5:\n",
    "        # Swap\n",
    "        try:\n",
    "            idx1, idx2 = random.sample(range(len(words)), 2)\n",
    "            words[idx1], words[idx2] = words[idx2], words[idx1]\n",
    "        except: return text\n",
    "    else:\n",
    "        # Delete\n",
    "        if len(words) > 1:\n",
    "            idx = random.randint(0, len(words)-1)\n",
    "            words.pop(idx)\n",
    "    return ' '.join(words)\n",
    "\n",
    "generated_data = []\n",
    "target_count = 2000\n",
    "original_data = df_clean.to_dict('records')\n",
    "generated_data.extend(original_data)\n",
    "\n",
    "print(f\"Original sentences: {len(original_data)}\")\n",
    "print(\"Generating new sentences...\")\n",
    "\n",
    "while len(generated_data) < target_count:\n",
    "    row = random.choice(original_data)\n",
    "    new_marma = augment_text(row['Marma'])\n",
    "    generated_data.append({\n",
    "        'English': row['English'],\n",
    "        'Bangla': row['Bangla'],\n",
    "        'Marma': new_marma\n",
    "    })\n",
    "\n",
    "# --- ‡¶ß‡¶æ‡¶™ ‡ß´: ‡¶´‡¶æ‡¶á‡¶≤ ‡¶∏‡ßá‡¶≠ ---\n",
    "output_filename = 'marma_dataset_2000.csv'\n",
    "df_final = pd.DataFrame(generated_data)\n",
    "# utf-8-sig ‡¶¶‡ßá‡¶ì‡ßü‡¶æ ‡¶π‡¶≤‡ßã ‡¶Ø‡¶æ‡¶§‡ßá ‡¶è‡¶ï‡ßç‡¶∏‡ßá‡¶≤‡ßá ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶†‡¶ø‡¶ï‡¶Æ‡¶§‡ßã ‡¶¶‡ßá‡¶ñ‡¶æ ‡¶Ø‡¶æ‡ßü\n",
    "df_final.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\n[SUCCESS] ‡¶ï‡¶æ‡¶ú ‡¶∏‡¶Æ‡ßç‡¶™‡¶®‡ßç‡¶®! {len(df_final)} ‡¶ü‡¶ø ‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø‡ßá‡¶∞ ‡¶´‡¶æ‡¶á‡¶≤ ‡¶§‡ßà‡¶∞‡¶ø ‡¶π‡ßü‡ßá‡¶õ‡ßá‡•§\")\n",
    "print(f\"‡¶´‡¶æ‡¶á‡¶≤‡ßá‡¶∞ ‡¶®‡¶æ‡¶Æ: {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fef3a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\walton\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeling shuru hocche...\n",
      "\n",
      "Sample Labeled Data:\n",
      "             English label\n",
      "0        I hate you.   neg\n",
      "1        I love you.   pos\n",
      "2      I am in love.   pos\n",
      "3        I am sorry.   neg\n",
      "4   I am very sorry.   neg\n",
      "5       How are you?   ntr\n",
      "6         I am fine.   pos\n",
      "7        I miss you.   neg\n",
      "8  That is very bad.   neg\n",
      "9     Get well soon.   pos\n",
      "\n",
      "[SUCCESS] Labeling complete! File saved as: marma_labeled_2000.csv\n",
      "New Statistics:\n",
      "label\n",
      "ntr    1155\n",
      "pos     575\n",
      "neg     270\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# 1. NLTK ‡¶è‡¶∞ ‡¶™‡ßç‡¶∞‡ßü‡ßã‡¶ú‡¶®‡ßÄ‡ßü ‡¶´‡¶æ‡¶á‡¶≤ ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶° (Colab ‡¶è ‡¶è‡¶ü‡¶ø ‡¶≤‡¶æ‡¶ó‡¶¨‡ßá)\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# 2. ‡¶´‡¶æ‡¶á‡¶≤ ‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡¶æ\n",
    "filename = 'marma_dataset_2000.csv'\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "# 3. Sentiment Analyzer ‡¶∏‡ßá‡¶ü‡¶Ü‡¶™\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# ‡¶´‡¶æ‡¶Ç‡¶∂‡¶®: ‡¶Ø‡¶æ ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶™‡ßú‡ßá ‡¶≤‡ßá‡¶¨‡ßá‡¶≤ ‡¶†‡¶ø‡¶ï ‡¶ï‡¶∞‡¶¨‡ßá\n",
    "def get_sentiment_label(text):\n",
    "    if not isinstance(text, str):\n",
    "        return 'ntr' # ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶®‡¶æ ‡¶•‡¶æ‡¶ï‡¶≤‡ßá ‡¶®‡¶ø‡¶â‡¶ü‡ßç‡¶∞‡¶æ‡¶≤\n",
    "    \n",
    "    # ‡¶∏‡ßç‡¶ï‡ßã‡¶∞ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ (-1 ‡¶•‡ßá‡¶ï‡ßá +1 ‡¶è‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá)\n",
    "    score = sia.polarity_scores(text)['compound']\n",
    "    \n",
    "    # ‡¶≤‡¶ú‡¶ø‡¶ï ‡¶∏‡ßá‡¶ü ‡¶ï‡¶∞‡¶æ\n",
    "    if score >= 0.05:\n",
    "        return 'pos'  # Positive\n",
    "    elif score <= -0.05:\n",
    "        return 'neg'  # Negative\n",
    "    else:\n",
    "        return 'ntr'  # Neutral\n",
    "\n",
    "# 4. English ‡¶ï‡¶≤‡¶æ‡¶Æ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶≤‡ßá‡¶¨‡ßá‡¶≤ ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶æ\n",
    "print(\"Labeling shuru hocche...\")\n",
    "# ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶´‡¶æ‡¶á‡¶≤‡ßá ‡¶á‡¶Ç‡¶∞‡ßá‡¶ú‡¶ø ‡¶ï‡¶≤‡¶æ‡¶Æ‡ßá‡¶∞ ‡¶®‡¶æ‡¶Æ 'English' (‡¶¨‡ßú ‡¶π‡¶æ‡¶§‡ßá‡¶∞ E) ‡¶Ü‡¶õ‡ßá ‡¶ï‡¶ø‡¶®‡¶æ ‡¶¶‡ßá‡¶ñ‡ßá ‡¶®‡ßá‡¶¨‡ßá‡¶®\n",
    "# ‡¶Ø‡¶¶‡¶ø ‡¶õ‡ßã‡¶ü ‡¶π‡¶æ‡¶§‡ßá‡¶∞ ‡¶π‡ßü ‡¶§‡¶¨‡ßá row['english'] ‡¶≤‡¶ø‡¶ñ‡¶¨‡ßá‡¶®\n",
    "df['label'] = df['English'].apply(get_sentiment_label)\n",
    "\n",
    "# 5. ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£ ‡¶¶‡ßá‡¶ñ‡¶æ (Check)\n",
    "print(\"\\nSample Labeled Data:\")\n",
    "print(df[['English', 'label']].head(10))\n",
    "\n",
    "# 6. ‡¶´‡¶æ‡¶á‡¶≤ ‡¶∏‡ßá‡¶≠ ‡¶ï‡¶∞‡¶æ\n",
    "output_file = 'marma_labeled_2000.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\n[SUCCESS] Labeling complete! File saved as: {output_file}\")\n",
    "print(\"New Statistics:\")\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b476d432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡¶ø‡¶Ç ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶π‡¶ö‡ßç‡¶õ‡ßá...\n",
      "‚úÖ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡¶ø‡¶Ç ‡¶∏‡¶Æ‡ßç‡¶™‡¶®‡ßç‡¶®! ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶è‡¶ï‡ßÅ‡¶∞‡ßá‡¶∏‡¶ø: 76.0%\n",
      "-------------------------------------------------------\n",
      "\n",
      "üëá ‡¶®‡¶ø‡¶ö‡ßá ‡¶Æ‡¶æ‡¶∞‡¶Æ‡¶æ ‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø ‡¶≤‡¶ø‡¶ñ‡ßÅ‡¶® (‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡¶§‡ßá 'exit' ‡¶≤‡¶ø‡¶ñ‡ßÅ‡¶®):\n",
      "‡¶∞‡ßá‡¶ú‡¶æ‡¶≤‡ßç‡¶ü: ‚ö™ NEUTRAL (‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£)\n",
      "‡¶∞‡ßá‡¶ú‡¶æ‡¶≤‡ßç‡¶ü: ‚ö™ NEUTRAL (‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£)\n",
      "‡¶∞‡ßá‡¶ú‡¶æ‡¶≤‡ßç‡¶ü: üü¢ POSITIVE (‡¶≠‡¶æ‡¶≤‡ßã/‡¶á‡¶§‡¶ø‡¶¨‡¶æ‡¶ö‡¶ï)\n",
      "‡¶∞‡ßá‡¶ú‡¶æ‡¶≤‡ßç‡¶ü: ‚ö™ NEUTRAL (‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£)\n",
      "‡¶∞‡ßá‡¶ú‡¶æ‡¶≤‡ßç‡¶ü: ‚ö™ NEUTRAL (‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£)\n",
      "‡¶∞‡ßá‡¶ú‡¶æ‡¶≤‡ßç‡¶ü: ‚ö™ NEUTRAL (‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£)\n",
      "‡¶∞‡ßá‡¶ú‡¶æ‡¶≤‡ßç‡¶ü: üî¥ NEGATIVE (‡¶ñ‡¶æ‡¶∞‡¶æ‡¶™/‡¶®‡ßá‡¶§‡¶ø‡¶¨‡¶æ‡¶ö‡¶ï)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ‡ßß. ‡¶´‡¶æ‡¶á‡¶≤ ‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡¶æ\n",
    "filename = 'marma_labeled_2000.csv'\n",
    "try:\n",
    "    df = pd.read_csv(filename)\n",
    "    df = df.dropna(subset=['Marma', 'label'])\n",
    "except:\n",
    "    print(\"Error: 'marma_labeled_2000.csv' ‡¶´‡¶æ‡¶á‡¶≤‡¶ü‡¶ø ‡¶™‡¶æ‡¶ì‡ßü‡¶æ ‡¶Ø‡¶æ‡¶ö‡ßç‡¶õ‡ßá ‡¶®‡¶æ!\")\n",
    "    exit()\n",
    "\n",
    "# ‡ß®. ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡¶ø‡¶Ç ‡¶ì ‡¶ü‡ßá‡¶∏‡ßç‡¶ü ‡¶°‡¶æ‡¶ü‡¶æ ‡¶≠‡¶æ‡¶ó ‡¶ï‡¶∞‡¶æ\n",
    "X = df['Marma']\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ‡ß©. ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶™‡¶æ‡¶á‡¶™‡¶≤‡¶æ‡¶á‡¶®\n",
    "model_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(1, 2))),\n",
    "    ('clf', LinearSVC())\n",
    "])\n",
    "\n",
    "# ‡ß™. ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡¶ø‡¶Ç\n",
    "print(\"ü§ñ ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡¶ø‡¶Ç ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶π‡¶ö‡ßç‡¶õ‡ßá...\")\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "accuracy = model_pipeline.score(X_test, y_test)\n",
    "print(f\"‚úÖ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡¶ø‡¶Ç ‡¶∏‡¶Æ‡ßç‡¶™‡¶®‡ßç‡¶®! ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶è‡¶ï‡ßÅ‡¶∞‡ßá‡¶∏‡¶ø: {round(accuracy * 100, 2)}%\")\n",
    "print(\"-------------------------------------------------------\")\n",
    "\n",
    "# ‡ß´. ‡¶≤‡¶æ‡¶á‡¶≠ ‡¶ü‡ßá‡¶∏‡ßç‡¶ü‡¶ø‡¶Ç ‡¶´‡¶æ‡¶Ç‡¶∂‡¶®\n",
    "def predict_marma_sentiment():\n",
    "    print(\"\\nüëá ‡¶®‡¶ø‡¶ö‡ßá ‡¶Æ‡¶æ‡¶∞‡¶Æ‡¶æ ‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø ‡¶≤‡¶ø‡¶ñ‡ßÅ‡¶® (‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡¶§‡ßá 'exit' ‡¶≤‡¶ø‡¶ñ‡ßÅ‡¶®):\")\n",
    "    \n",
    "    while True:\n",
    "        # ‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶∂‡ßÅ‡¶ß‡ßÅ ‡¶™‡ßç‡¶∞‡¶Æ‡ßç‡¶™‡¶ü ‡¶•‡¶æ‡¶ï‡¶¨‡ßá, ‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø ‡¶®‡¶æ\n",
    "        user_text = input(\"\\nüìù ‡¶Æ‡¶æ‡¶∞‡¶Æ‡¶æ ‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø: \") \n",
    "        \n",
    "        if user_text.lower() == 'exit':\n",
    "            print(\"‡¶ß‡¶®‡ßç‡¶Ø‡¶¨‡¶æ‡¶¶! ‡¶™‡ßç‡¶∞‡ßã‡¶ó‡ßç‡¶∞‡¶æ‡¶Æ ‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡¶æ ‡¶π‡¶≤‡ßã‡•§\")\n",
    "            break\n",
    "        \n",
    "        if not user_text.strip():\n",
    "            continue\n",
    "\n",
    "        # ‡¶™‡ßç‡¶∞‡ßá‡¶°‡¶ø‡¶ï‡¶∂‡¶®\n",
    "        try:\n",
    "            prediction = model_pipeline.predict([user_text])[0]\n",
    "            \n",
    "            result_map = {\n",
    "                'pos': 'üü¢ POSITIVE (‡¶≠‡¶æ‡¶≤‡ßã/‡¶á‡¶§‡¶ø‡¶¨‡¶æ‡¶ö‡¶ï)',\n",
    "                'neg': 'üî¥ NEGATIVE (‡¶ñ‡¶æ‡¶∞‡¶æ‡¶™/‡¶®‡ßá‡¶§‡¶ø‡¶¨‡¶æ‡¶ö‡¶ï)',\n",
    "                'ntr': '‚ö™ NEUTRAL (‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£)'\n",
    "            }\n",
    "            print(f\"‡¶∞‡ßá‡¶ú‡¶æ‡¶≤‡ßç‡¶ü: {result_map.get(prediction, prediction)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "\n",
    "# ‡¶´‡¶æ‡¶Ç‡¶∂‡¶® ‡¶ï‡¶≤ (‡¶¨‡¶æ‡¶®‡¶æ‡¶® ‡¶†‡¶ø‡¶ï ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡ßá‡¶õ‡ßá)\n",
    "predict_marma_sentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c070312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶°‡¶≠‡¶æ‡¶®‡ßç‡¶∏‡¶° ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡¶ø‡¶Ç ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶π‡¶ö‡ßç‡¶õ‡ßá...\n",
      "üî• ‡¶®‡¶§‡ßÅ‡¶® ‡¶è‡¶ï‡ßÅ‡¶∞‡ßá‡¶∏‡¶ø: 99.0%\n",
      "\n",
      "‡¶¨‡¶ø‡¶∏‡ßç‡¶§‡¶æ‡¶∞‡¶ø‡¶§ ‡¶∞‡¶ø‡¶™‡ßã‡¶∞‡ßç‡¶ü:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       1.00      1.00      1.00        38\n",
      "         ntr       0.98      1.00      0.99       170\n",
      "         pos       1.00      0.97      0.98        92\n",
      "\n",
      "    accuracy                           0.99       300\n",
      "   macro avg       0.99      0.99      0.99       300\n",
      "weighted avg       0.99      0.99      0.99       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# ‡ßß. ‡¶´‡¶æ‡¶á‡¶≤ ‡¶≤‡ßã‡¶°\n",
    "filename = 'marma_labeled_2000.csv'\n",
    "df = pd.read_csv(filename)\n",
    "df = df.dropna(subset=['Marma', 'label'])\n",
    "\n",
    "# ‡ß®. ‡¶°‡¶æ‡¶ü‡¶æ ‡¶≠‡¶æ‡¶ó ‡¶ï‡¶∞‡¶æ\n",
    "X = df['Marma']\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42) \n",
    "# test_size=0.15 ‡¶Æ‡¶æ‡¶®‡ßá ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡¶ø‡¶Ç‡ßü‡ßá ‡¶Ü‡¶∞‡¶ì ‡¶¨‡ßá‡¶∂‡¶ø ‡¶°‡¶æ‡¶ü‡¶æ (‡ßÆ‡ß´%) ‡¶¶‡¶ø‡¶ö‡ßç‡¶õ‡¶ø‡•§\n",
    "\n",
    "# ‡ß©. ‡¶™‡¶æ‡¶ì‡ßü‡¶æ‡¶∞‡¶´‡ßÅ‡¶≤ ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶™‡¶æ‡¶á‡¶™‡¶≤‡¶æ‡¶á‡¶®\n",
    "model_pipeline = Pipeline([\n",
    "    # ngram_range=(1, 3) ‡¶Æ‡¶æ‡¶®‡ßá ‡¶∏‡ßá ‡¶è‡¶ñ‡¶® ‡ß© ‡¶∂‡¶¨‡ßç‡¶¶‡ßá‡¶∞ ‡¶´‡ßç‡¶∞‡ßá‡¶ú‡¶ì ‡¶¨‡ßÅ‡¶ù‡¶¨‡ßá\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(1, 3), analyzer='char_wb', min_df=1)), \n",
    "    # C=10 ‡¶è‡¶¨‡¶Ç class_weight='balanced' ‡¶è‡¶ï‡ßÅ‡¶∞‡ßá‡¶∏‡¶ø ‡¶¨‡¶æ‡ßú‡¶æ‡¶§‡ßá ‡¶∏‡¶æ‡¶π‡¶æ‡¶Ø‡ßç‡¶Ø ‡¶ï‡¶∞‡ßá\n",
    "    ('clf', LinearSVC(C=10, class_weight='balanced', max_iter=10000)) \n",
    "])\n",
    "\n",
    "# ‡ß™. ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡¶ø‡¶Ç\n",
    "print(\"üöÄ ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶°‡¶≠‡¶æ‡¶®‡ßç‡¶∏‡¶° ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡¶ø‡¶Ç ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶π‡¶ö‡ßç‡¶õ‡ßá...\")\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# ‡ß´. ‡¶∞‡ßá‡¶ú‡¶æ‡¶≤‡ßç‡¶ü ‡¶ö‡ßá‡¶ï\n",
    "predictions = model_pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "print(f\"üî• ‡¶®‡¶§‡ßÅ‡¶® ‡¶è‡¶ï‡ßÅ‡¶∞‡ßá‡¶∏‡¶ø: {round(accuracy * 100, 2)}%\")\n",
    "print(\"\\n‡¶¨‡¶ø‡¶∏‡ßç‡¶§‡¶æ‡¶∞‡¶ø‡¶§ ‡¶∞‡¶ø‡¶™‡ßã‡¶∞‡ßç‡¶ü:\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc82ba81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Counts:\n",
      "label\n",
      "ntr    1155\n",
      "pos     575\n",
      "neg     270\n",
      "Name: count, dtype: int64\n",
      "‚¨ÜÔ∏è Augmenting POS: 575 -> 667\n",
      "‚¨ÜÔ∏è Augmenting NEG: 270 -> 667\n",
      "‚¨áÔ∏è Downsampling NTR: 1155 -> 666\n",
      "\n",
      "------------------------------------------------\n",
      "[SUCCESS] ‡¶®‡¶§‡ßÅ‡¶® ‡¶¨‡ßç‡¶Ø‡¶æ‡¶≤‡ßá‡¶®‡ßç‡¶∏‡¶° ‡¶´‡¶æ‡¶á‡¶≤ ‡¶§‡ßà‡¶∞‡¶ø ‡¶π‡ßü‡ßá‡¶õ‡ßá: marma_balanced_2000.csv\n",
      "New Class Distribution:\n",
      "label\n",
      "pos    667\n",
      "neg    667\n",
      "ntr    666\n",
      "Name: count, dtype: int64\n",
      "Total: 2000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# ‡ßß. ‡¶´‡¶æ‡¶á‡¶≤ ‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡¶æ\n",
    "filename = 'marma_labeled_2000.csv'  # ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶® ‡¶´‡¶æ‡¶á‡¶≤‡ßá‡¶∞ ‡¶®‡¶æ‡¶Æ\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(filename)\n",
    "    # ‡¶´‡¶æ‡¶ï‡¶æ ‡¶°‡¶æ‡¶ü‡¶æ ‡¶•‡¶æ‡¶ï‡¶≤‡ßá ‡¶´‡ßá‡¶≤‡ßá ‡¶¶‡¶ø‡¶ö‡ßç‡¶õ‡¶ø\n",
    "    df = df.dropna(subset=['Marma', 'label'])\n",
    "except:\n",
    "    print(\"Error: ‡¶´‡¶æ‡¶á‡¶≤‡¶ü‡¶ø ‡¶™‡¶æ‡¶ì‡ßü‡¶æ ‡¶Ø‡¶æ‡¶ö‡ßç‡¶õ‡ßá ‡¶®‡¶æ! ‡¶´‡¶æ‡¶á‡¶≤‡ßá‡¶∞ ‡¶®‡¶æ‡¶Æ ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡ßÅ‡¶®‡•§\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Original Data Counts:\\n{df['label'].value_counts()}\")\n",
    "\n",
    "# ‡ß®. ‡¶Ö‡¶ó‡¶Æ‡ßá‡¶®‡ßç‡¶ü‡ßá‡¶∂‡¶® ‡¶´‡¶æ‡¶Ç‡¶∂‡¶® (‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø ‡¶Æ‡¶°‡¶ø‡¶´‡¶æ‡¶á ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø)\n",
    "def augment_text(text):\n",
    "    text = str(text)\n",
    "    words = text.split()\n",
    "    if len(words) < 3: return text\n",
    "    \n",
    "    # ‡ß´‡ß¶% ‡¶ö‡¶æ‡¶®‡ßç‡¶∏ ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶Ö‡¶¶‡¶≤‡¶¨‡¶¶‡¶≤ ‡¶ï‡¶∞‡¶æ‡¶∞, ‡¶¨‡¶æ‡¶ï‡¶ø ‡ß´‡ß¶% ‡¶ö‡¶æ‡¶®‡ßç‡¶∏ ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶°‡¶ø‡¶≤‡¶ø‡¶ü ‡¶ï‡¶∞‡¶æ‡¶∞\n",
    "    if random.random() > 0.5:\n",
    "        try:\n",
    "            # Swap (‡¶Ö‡¶¶‡¶≤‡¶¨‡¶¶‡¶≤)\n",
    "            idx1, idx2 = random.sample(range(len(words)), 2)\n",
    "            words[idx1], words[idx2] = words[idx2], words[idx1]\n",
    "        except: return text\n",
    "    else:\n",
    "        # Delete (‡¶Æ‡ßÅ‡¶õ‡ßá ‡¶´‡ßá‡¶≤‡¶æ)\n",
    "        if len(words) > 1:\n",
    "            idx = random.randint(0, len(words)-1)\n",
    "            words.pop(idx)\n",
    "    return ' '.join(words)\n",
    "\n",
    "# ‡ß©. ‡¶Ü‡¶≤‡¶æ‡¶¶‡¶æ ‡¶Ü‡¶≤‡¶æ‡¶¶‡¶æ ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡ßá ‡¶≠‡¶æ‡¶ó ‡¶ï‡¶∞‡¶æ\n",
    "df_pos = df[df['label'] == 'pos']\n",
    "df_neg = df[df['label'] == 'neg']\n",
    "df_ntr = df[df['label'] == 'ntr']\n",
    "\n",
    "# ‡ß™. ‡¶¨‡ßç‡¶Ø‡¶æ‡¶≤‡ßá‡¶®‡ßç‡¶∏ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶≤‡¶ú‡¶ø‡¶ï\n",
    "def balance_class(df_class, target_count, label_name):\n",
    "    current_count = len(df_class)\n",
    "    \n",
    "    if current_count >= target_count:\n",
    "        # ‡¶Ø‡¶¶‡¶ø ‡¶ü‡¶æ‡¶∞‡ßç‡¶ó‡ßá‡¶ü‡ßá‡¶∞ ‡¶ö‡ßá‡ßü‡ßá ‡¶¨‡ßá‡¶∂‡¶ø ‡¶•‡¶æ‡¶ï‡ßá, ‡¶§‡¶æ‡¶π‡¶≤‡ßá ‡¶ï‡¶Æ‡¶ø‡ßü‡ßá ‡¶´‡ßá‡¶≤‡¶¨ (Downsample)\n",
    "        print(f\"‚¨áÔ∏è Downsampling {label_name}: {current_count} -> {target_count}\")\n",
    "        return resample(df_class, n_samples=target_count, replace=False, random_state=42)\n",
    "    \n",
    "    else:\n",
    "        # ‡¶Ø‡¶¶‡¶ø ‡¶ü‡¶æ‡¶∞‡ßç‡¶ó‡ßá‡¶ü‡ßá‡¶∞ ‡¶ö‡ßá‡ßü‡ßá ‡¶ï‡¶Æ ‡¶•‡¶æ‡¶ï‡ßá, ‡¶§‡¶æ‡¶π‡¶≤‡ßá ‡¶¨‡¶æ‡ßú‡¶ø‡ßü‡ßá ‡¶´‡ßá‡¶≤‡¶¨ (Upsample / Augment)\n",
    "        print(f\"‚¨ÜÔ∏è Augmenting {label_name}: {current_count} -> {target_count}\")\n",
    "        \n",
    "        # ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá ‡¶Ö‡¶∞‡¶ø‡¶ú‡¶ø‡¶®‡¶æ‡¶≤ ‡¶°‡¶æ‡¶ü‡¶æ ‡¶∞‡¶æ‡¶ñ‡¶¨\n",
    "        augmented_data = df_class.to_dict('records')\n",
    "        needed = target_count - current_count\n",
    "        \n",
    "        # ‡¶Ø‡¶§‡¶ó‡ßÅ‡¶≤‡ßã ‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞ ‡¶§‡¶§‡¶ó‡ßÅ‡¶≤‡ßã ‡¶®‡¶§‡ßÅ‡¶® ‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø ‡¶¨‡¶æ‡¶®‡¶æ‡¶¨\n",
    "        source_data = df_class.to_dict('records')\n",
    "        while len(augmented_data) < target_count:\n",
    "            row = random.choice(source_data)\n",
    "            new_marma = augment_text(row['Marma'])\n",
    "            \n",
    "            # ‡¶®‡¶§‡ßÅ‡¶® ‡¶°‡¶æ‡¶ü‡¶æ ‡¶≤‡¶ø‡¶∏‡ßç‡¶ü‡ßá ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡¶æ\n",
    "            augmented_data.append({\n",
    "                'English': row['English'],\n",
    "                'Bangla': row['Bangla'],\n",
    "                'Marma': new_marma,\n",
    "                'label': row['label'] # ‡¶≤‡ßá‡¶¨‡ßá‡¶≤ ‡¶è‡¶ï‡¶á ‡¶•‡¶æ‡¶ï‡¶¨‡ßá\n",
    "            })\n",
    "            \n",
    "        return pd.DataFrame(augmented_data)\n",
    "\n",
    "# ‡ß´. ‡¶ü‡¶æ‡¶∞‡ßç‡¶ó‡ßá‡¶ü ‡¶∏‡ßá‡¶ü ‡¶ï‡¶∞‡¶æ (‡¶Æ‡ßã‡¶ü ‡ß®‡ß¶‡ß¶‡ß¶)\n",
    "# ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶™‡¶ú‡¶ø‡¶ü‡¶ø‡¶≠ ‡¶ì ‡¶®‡ßá‡¶ó‡ßá‡¶ü‡¶ø‡¶≠‡¶ï‡ßá ‡¶è‡¶ï‡¶ü‡ßÅ ‡¶¨‡ßá‡¶∂‡¶ø ‡¶™‡ßç‡¶∞‡¶æ‡¶ß‡¶æ‡¶®‡ßç‡¶Ø ‡¶¶‡¶ø‡¶ö‡ßç‡¶õ‡¶ø ‡¶Ø‡¶æ‡¶§‡ßá ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶≠‡¶æ‡¶≤‡ßã ‡¶∂‡¶ø‡¶ñ‡ßá\n",
    "df_pos_balanced = balance_class(df_pos, 667, 'POS')\n",
    "df_neg_balanced = balance_class(df_neg, 667, 'NEG')\n",
    "df_ntr_balanced = balance_class(df_ntr, 666, 'NTR')\n",
    "\n",
    "# ‡ß¨. ‡¶∏‡¶¨ ‡¶ú‡ßã‡ßú‡¶æ ‡¶≤‡¶æ‡¶ó‡¶æ‡¶®‡ßã\n",
    "df_final = pd.concat([df_pos_balanced, df_neg_balanced, df_ntr_balanced])\n",
    "\n",
    "# ‡¶°‡¶æ‡¶ü‡¶æ ‡¶∂‡¶æ‡¶´‡¶≤ ‡¶ï‡¶∞‡¶æ (‡¶è‡¶≤‡ßã‡¶Æ‡ßá‡¶≤‡ßã ‡¶ï‡¶∞‡¶æ)\n",
    "df_final = df_final.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# ‡ß≠. ‡¶∏‡ßá‡¶≠ ‡¶ï‡¶∞‡¶æ\n",
    "output_filename = 'marma_balanced_2000.csv'\n",
    "df_final.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"\\n------------------------------------------------\")\n",
    "print(f\"[SUCCESS] ‡¶®‡¶§‡ßÅ‡¶® ‡¶¨‡ßç‡¶Ø‡¶æ‡¶≤‡ßá‡¶®‡ßç‡¶∏‡¶° ‡¶´‡¶æ‡¶á‡¶≤ ‡¶§‡ßà‡¶∞‡¶ø ‡¶π‡ßü‡ßá‡¶õ‡ßá: {output_filename}\")\n",
    "print(\"New Class Distribution:\")\n",
    "print(df_final['label'].value_counts())\n",
    "print(\"Total:\", len(df_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbe5fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ‡¶´‡¶æ‡¶á‡¶≤ ‡¶≤‡ßã‡¶° ‡¶π‡ßü‡ßá‡¶õ‡ßá: marma_balanced_2000.csv\n",
      "‡¶Æ‡ßã‡¶ü ‡¶°‡¶æ‡¶ü‡¶æ: 2000\n",
      "\n",
      "ü§ñ ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡¶ø‡¶Ç ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶π‡¶ö‡ßç‡¶õ‡ßá (Advanced Mode)...\n",
      "-------------------------------------------------------\n",
      "üèÜ ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶® ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶è‡¶ï‡ßÅ‡¶∞‡ßá‡¶∏‡¶ø: 98.67%\n",
      "-------------------------------------------------------\n",
      "\n",
      "‡¶°‡¶ø‡¶ü‡ßá‡¶á‡¶≤‡¶° ‡¶∞‡¶ø‡¶™‡ßã‡¶∞‡ßç‡¶ü:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       1.00      1.00      1.00       100\n",
      "         ntr       0.99      0.97      0.98       100\n",
      "         pos       0.97      0.99      0.98       100\n",
      "\n",
      "    accuracy                           0.99       300\n",
      "   macro avg       0.99      0.99      0.99       300\n",
      "weighted avg       0.99      0.99      0.99       300\n",
      "\n",
      "\n",
      "Confusion Matrix (‡¶ï‡ßã‡¶®‡¶ü‡¶æ ‡¶Ü‡¶∏‡¶≤‡ßá ‡¶ï‡ßÄ, ‡¶Ü‡¶∞ ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶ï‡ßÄ ‡¶≠‡ßá‡¶¨‡ßá‡¶õ‡ßá):\n",
      "[[ 99   0   1]\n",
      " [  0 100   0]\n",
      " [  3   0  97]]\n",
      "\n",
      "üëá ‡¶ü‡ßá‡¶∏‡ßç‡¶ü ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Æ‡¶æ‡¶∞‡¶Æ‡¶æ ‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø ‡¶≤‡¶ø‡¶ñ‡ßÅ‡¶® (‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡¶§‡ßá 'exit' ‡¶≤‡¶ø‡¶ñ‡ßÅ‡¶®):\n",
      "‡¶∞‡ßá‡¶ú‡¶æ‡¶≤‡ßç‡¶ü: üî¥ NEGATIVE (‡¶ñ‡¶æ‡¶∞‡¶æ‡¶™)\n",
      "‡¶∞‡ßá‡¶ú‡¶æ‡¶≤‡ßç‡¶ü: ‚ö™ NEUTRAL (‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£)\n",
      "‡¶∞‡ßá‡¶ú‡¶æ‡¶≤‡ßç‡¶ü: ‚ö™ NEUTRAL (‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£)\n",
      "‡¶∞‡ßá‡¶ú‡¶æ‡¶≤‡ßç‡¶ü: üî¥ NEGATIVE (‡¶ñ‡¶æ‡¶∞‡¶æ‡¶™)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# ‡ßß. ‡¶¨‡ßç‡¶Ø‡¶æ‡¶≤‡ßá‡¶®‡ßç‡¶∏ ‡¶ï‡¶∞‡¶æ ‡¶´‡¶æ‡¶á‡¶≤ ‡¶≤‡ßã‡¶°\n",
    "filename = 'marma_balanced_2000.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(filename)\n",
    "    df = df.dropna(subset=['Marma', 'label'])\n",
    "    print(f\"‚úÖ ‡¶´‡¶æ‡¶á‡¶≤ ‡¶≤‡ßã‡¶° ‡¶π‡ßü‡ßá‡¶õ‡ßá: {filename}\")\n",
    "    print(f\"‡¶Æ‡ßã‡¶ü ‡¶°‡¶æ‡¶ü‡¶æ: {len(df)}\")\n",
    "except:\n",
    "    print(f\"Error: '{filename}' ‡¶´‡¶æ‡¶á‡¶≤‡¶ü‡¶ø ‡¶™‡¶æ‡¶ì‡ßü‡¶æ ‡¶Ø‡¶æ‡¶ö‡ßç‡¶õ‡ßá ‡¶®‡¶æ‡•§ ‡¶Ü‡¶ó‡ßá‡¶∞ ‡¶ï‡ßã‡¶°‡¶ü‡¶ø ‡¶∞‡¶æ‡¶® ‡¶ï‡¶∞‡ßá ‡¶´‡¶æ‡¶á‡¶≤‡¶ü‡¶ø ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßÅ‡¶®‡•§\")\n",
    "    exit()\n",
    "\n",
    "# ‡ß®. ‡¶á‡¶®‡¶™‡ßÅ‡¶ü ‡¶ì ‡¶Ü‡¶â‡¶ü‡¶™‡ßÅ‡¶ü ‡¶≠‡¶æ‡¶ó ‡¶ï‡¶∞‡¶æ\n",
    "X = df['Marma']\n",
    "y = df['label']\n",
    "\n",
    "# ‡¶°‡¶æ‡¶ü‡¶æ ‡¶∏‡ßç‡¶™‡ßç‡¶≤‡¶ø‡¶ü (Stratify ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶õ‡¶ø ‡¶Ø‡¶æ‡¶§‡ßá ‡¶ü‡ßá‡¶∏‡ßç‡¶ü ‡¶°‡¶æ‡¶ü‡¶æ‡ßü ‡¶∏‡¶¨ ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡ßá‡¶∞ ‡¶∏‡¶Æ‡¶æ‡¶® ‡¶Ö‡¶Ç‡¶∂ ‡¶•‡¶æ‡¶ï‡ßá)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "\n",
    "# ‡ß©. ‡¶™‡¶æ‡¶ì‡ßü‡¶æ‡¶∞‡¶´‡ßÅ‡¶≤ ‡¶™‡¶æ‡¶á‡¶™‡¶≤‡¶æ‡¶á‡¶® ‡¶§‡ßà‡¶∞‡¶ø\n",
    "# ngram_range=(1, 3) -> ‡ßß ‡¶∂‡¶¨‡ßç‡¶¶, ‡ß® ‡¶∂‡¶¨‡ßç‡¶¶ ‡¶è‡¶¨‡¶Ç ‡ß© ‡¶∂‡¶¨‡ßç‡¶¶‡ßá‡¶∞ ‡¶ï‡¶Æ‡ßç‡¶¨‡¶ø‡¶®‡ßá‡¶∂‡¶® ‡¶¶‡ßá‡¶ñ‡¶¨‡ßá\n",
    "# analyzer='char_wb' -> ‡¶∂‡¶¨‡ßç‡¶¶‡ßá‡¶∞ ‡¶≠‡ßá‡¶§‡¶∞‡ßá‡¶∞ ‡¶Ö‡¶ï‡ßç‡¶∑‡¶∞‡ßá‡¶∞ ‡¶™‡ßç‡¶Ø‡¶æ‡¶ü‡¶æ‡¶∞‡ßç‡¶®‡¶ì ‡¶¶‡ßá‡¶ñ‡¶¨‡ßá (‡¶Æ‡¶æ‡¶∞‡¶Æ‡¶æ ‡¶≠‡¶æ‡¶∑‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶ñ‡ßÅ‡¶¨ ‡¶ï‡¶æ‡¶∞‡ßç‡¶Ø‡¶ï‡¶∞‡ßÄ)\n",
    "model_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(1, 4), analyzer='char_wb', min_df=1)),\n",
    "    ('clf', LinearSVC(C=10, max_iter=10000, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# ‡ß™. ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡¶ø‡¶Ç\n",
    "print(\"\\nü§ñ ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶ü‡ßç‡¶∞‡ßá‡¶®‡¶ø‡¶Ç ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶π‡¶ö‡ßç‡¶õ‡ßá (Advanced Mode)...\")\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# ‡ß´. ‡¶è‡¶ï‡ßÅ‡¶∞‡ßá‡¶∏‡¶ø ‡¶è‡¶¨‡¶Ç ‡¶∞‡¶ø‡¶™‡ßã‡¶∞‡ßç‡¶ü\n",
    "predictions = model_pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "print(\"-------------------------------------------------------\")\n",
    "print(f\"üèÜ ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶® ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶è‡¶ï‡ßÅ‡¶∞‡ßá‡¶∏‡¶ø: {round(accuracy * 100, 2)}%\")\n",
    "print(\"-------------------------------------------------------\")\n",
    "\n",
    "print(\"\\n‡¶°‡¶ø‡¶ü‡ßá‡¶á‡¶≤‡¶° ‡¶∞‡¶ø‡¶™‡ßã‡¶∞‡ßç‡¶ü:\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "print(\"\\nConfusion Matrix (‡¶ï‡ßã‡¶®‡¶ü‡¶æ ‡¶Ü‡¶∏‡¶≤‡ßá ‡¶ï‡ßÄ, ‡¶Ü‡¶∞ ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶ï‡ßÄ ‡¶≠‡ßá‡¶¨‡ßá‡¶õ‡ßá):\")\n",
    "print(confusion_matrix(y_test, predictions, labels=['pos', 'neg', 'ntr']))\n",
    "\n",
    "# ‡ß¨. ‡¶≤‡¶æ‡¶á‡¶≠ ‡¶ü‡ßá‡¶∏‡ßç‡¶ü ‡¶´‡¶æ‡¶Ç‡¶∂‡¶®\n",
    "def predict_now():\n",
    "    print(\"\\nüëá ‡¶ü‡ßá‡¶∏‡ßç‡¶ü ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Æ‡¶æ‡¶∞‡¶Æ‡¶æ ‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø ‡¶≤‡¶ø‡¶ñ‡ßÅ‡¶® (‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡¶§‡ßá 'exit' ‡¶≤‡¶ø‡¶ñ‡ßÅ‡¶®):\")\n",
    "    \n",
    "    label_map = {\n",
    "        'pos': 'üü¢ POSITIVE (‡¶≠‡¶æ‡¶≤‡ßã)',\n",
    "        'neg': 'üî¥ NEGATIVE (‡¶ñ‡¶æ‡¶∞‡¶æ‡¶™)',\n",
    "        'ntr': '‚ö™ NEUTRAL (‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£)'\n",
    "    }\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nüìù ‡¶¨‡¶æ‡¶ï‡ßç‡¶Ø: \")\n",
    "        \n",
    "        if user_input.lower() in ['exit', 'quit', 'stop']:\n",
    "            print(\"‡¶ß‡¶®‡ßç‡¶Ø‡¶¨‡¶æ‡¶¶! ‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡¶æ ‡¶π‡¶≤‡ßã‡•§\")\n",
    "            break\n",
    "            \n",
    "        if not user_input.strip():\n",
    "            continue\n",
    "            \n",
    "        pred = model_pipeline.predict([user_input])[0]\n",
    "        print(f\"‡¶∞‡ßá‡¶ú‡¶æ‡¶≤‡ßç‡¶ü: {label_map.get(pred, pred)}\")\n",
    "\n",
    "# ‡¶ü‡ßá‡¶∏‡ßç‡¶ü ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶ï‡¶∞‡ßÅ‡¶®\n",
    "predict_now()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
